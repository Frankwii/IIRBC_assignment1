{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2e7419af-3acf-42b4-9195-f11fe53499c3",
      "metadata": {},
      "source": [
        "# **11762 Content-Based Image Retrieval**\n",
        "## Master's Degree in Intelligent Systems\n",
        "### University of the Balearic Islands\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b751a557-1915-4df9-985a-e9a4b0d6f279",
      "metadata": {},
      "source": [
        "##### Write in the following the names of the members of the group:\n",
        "- **Member 1:** John Smith 1\n",
        "- **Member 2:** John Smith 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84f7308-0949-418a-b46e-a110b2889a9a",
      "metadata": {},
      "source": [
        "# **Instructions**\n",
        "\n",
        "**Do not delete any of the provided cells or functions**. Write your code in the indicated sections. You may add new cells or functions as needed to complete your work.  \n",
        "\n",
        "Along with this assignment, please submit a comprehensive **report** in PDF format explaining your implemented solutions. The report should include, for example:\n",
        "\n",
        "- **Technical Explanation**: Clearly describe the algorithms and techniques used, including relevant code snippets.\n",
        "- **Results**: Present your results in a clear and concise manner. Use visualizations such as graphs or tables to enhance understanding.\n",
        "- **Analysis**: Interpret your results and discuss their significance. Consider, for instance, the following questions:\n",
        "  - Are the results as expected? Why or why not?\n",
        "  - What factors might have influenced the results?\n",
        "- **Conclusions**: Summarize your findings and provide overall conclusions about your project.\n",
        "- ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dcb110d-c36a-4c9a-9212-b418dc3ae3e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute this cell to make sure \n",
        "# that external modules are reloaded\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a100e19-45dd-4d3a-b8d7-50d2c7ae3e41",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill the following variable with\n",
        "# the path to the Holidays (or Mini) dataset\n",
        "dataset_dir = 'path_to_INRIAHolidays_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "238f0831-31b1-431a-9abf-7b546bf82df1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup code for this assignment\n",
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import skimage.feature as skfeat\n",
        "import zipfile\n",
        "from holidays_dataset_handler import HolidaysDatasetHandler\n",
        "\n",
        "import assignment1 as a1\n",
        "\n",
        "# Configuring Matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62a769a0-6d2f-48ef-8596-88dcbacd4b4c",
      "metadata": {
        "editable": true,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2bb9e784ff104b808e847f731952b9c5",
          "grade": false,
          "grade_id": "cell-4067acd94a6d9403",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "# **Introduction**\n",
        "In this assignment, you will first learn the tools that will be used throughout the labs. Then, you will implement and evaluate various image description methods to develop a simple Content-Based Image Retrieval (CBIR) system.\n",
        "\n",
        "### Tools\n",
        "Several tools and libraries will be used throughout the semester, including:\n",
        "\n",
        "*   [Python 3](https://docs.python.org/3/tutorial/)\n",
        "*   [NumPy](https://docs.scipy.org/doc/numpy/user/quickstart.html)\n",
        "*   [OpenCV](https://docs.opencv.org/master/d6/d00/tutorial_py_root.html)\n",
        "\n",
        "It is important to familiarize yourself with these tools. If you are not already familiar with them, please follow the links above and explore the tutorials. Quickstart guides are also available on the course website on [Aula Digital](https://ad.uib.es/).\n",
        "\n",
        "### The INRIA Holidays Dataset\n",
        "During this course, we will primarily use the INRIA Holidays dataset. This dataset consists of a collection of personal holiday photos from the authors, along with additional images purposefully taken to evaluate robustness to image transformations such as rotations, viewpoint changes, illumination variations, blurring, and more. The dataset includes a wide variety of scene types (natural landscapes, man-made structures, water, fire effects, etc.), with all images available in high resolution.\n",
        "\n",
        "The dataset is organized into **500 groups**, each representing a distinct scene or object. For evaluation, the first image in each group serves as a **query image**, and the other images in the group are considered **relevant images** for that query. For example:\n",
        "\n",
        "- *100900.jpg* is the query image.\n",
        "- *100901.jpg* and *100902.jpg* are relevant images from the same group.\n",
        "\n",
        "The dataset comprises a total of **1491 images**, including:\n",
        "\n",
        "- **500 queries** (one for each group).\n",
        "- **991 relevant images**.\n",
        "\n",
        "### The INRIA Holidays Dataset (MINI)\n",
        "To facilitate development, a reduced version of the INRIA Holidays dataset, called the *mini* dataset, is provided. **You should mainly use this version for this assignment.** It contains **50 images**, including:\n",
        "\n",
        "- **19 queries**.\n",
        "- **31 relevant images**.\n",
        "\n",
        "> **Note**: Both datasets can be downloaded from [Aula Digital](https://ad.uib.es/). After downloading, unzip the files into a writable directory on your machine. Ensure that the corresponding variable at the beginning of this notebook is updated with the correct path to the dataset.\n",
        "\n",
        "The datasets follow this structure:\n",
        "```\n",
        "holidays/\n",
        "  \u251c\u2500\u2500 holidays_images.dat\n",
        "  \u251c\u2500\u2500 images/\n",
        "  \u2502     \u251c\u2500\u2500 100000.jpg\n",
        "  \u2502     \u251c\u2500\u2500 100001.jpg\n",
        "  \u2502     \u2514\u2500\u2500 ...\n",
        "  \u2514\u2500\u2500 features/\n",
        "        \u251c\u2500\u2500 100000.siftgeo\n",
        "        \u251c\u2500\u2500 100001.siftgeo\n",
        "        \u2514\u2500\u2500 ...\n",
        "  ...\n",
        "```\n",
        "where:\n",
        "\n",
        "- **`holidays_images.dat`**: This file lists the filenames of all images in the dataset.\n",
        "- **`images/`**: Contains the high-resolution image files.\n",
        "- **`features/`**: Includes the `.siftgeo` files, which store a set of extracted features (keypoints and descriptors) using SIFT for each image. Ensure these files are present if feature-based operations are required.\n",
        "\n",
        "The `holidays_images.dat` file can also serve as ground truth for performance evaluation of retrieval systems, as explained in a later section.\n",
        "\n",
        "### Using the *HolidaysDatasetHandler* Class\n",
        "The `HolidaysDatasetHandler` class is designed to help you manage and interact with the INRIA Holidays dataset efficiently. It provides functionality for loading images, extracting keypoints and descriptors, and calculating evaluation metrics like Average Precision (AP) and Mean Average Precision (mAP).\n",
        "\n",
        "**Key Features**\n",
        "- Load and retrieve query/database images.\n",
        "- Access ground truth data for evaluation.\n",
        "- Load precomputed keypoints and descriptors for feature-based tasks.\n",
        "- Compute AP and mAP for query evaluation.\n",
        "\n",
        "#### Example Usage\n",
        "**Importing and Initializing**\n",
        "```python\n",
        "# Initialize the dataset handler\n",
        "dataset = HolidaysDatasetHandler(dataset_dir, load_features=True)\n",
        "```\n",
        "\n",
        "**Loading and Displaying an Image**\n",
        "```python\n",
        "# Get an image from the dataset\n",
        "image_name = \"100900.jpg\"\n",
        "image = dataset.get_image(image_name)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image)\n",
        "plt.axis('off')  # Optional, to hide axes\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Accesing Query and Database Images**\n",
        "```python\n",
        "# List all query images\n",
        "query_images = dataset.get_query_images()\n",
        "print(f\"Number of query images: {len(query_images)}\")\n",
        "\n",
        "# List all database images\n",
        "database_images = dataset.get_database_images()\n",
        "print(f\"Number of database images: {len(database_images)}\")\n",
        "```\n",
        "\n",
        "**Loading Keypoints and Descriptors**\n",
        "```python\n",
        "# Load keypoints and descriptors for a specific image\n",
        "kps = dataset.get_kps(\"100900.jpg\")\n",
        "descs = dataset.get_descriptors(\"100900.jpg\")\n",
        "\n",
        "# Print the first keypoint and its descriptor\n",
        "print(f\"Keypoint: {kps[0].pt}, Descriptor shape: {descs.shape}\")\n",
        "```\n",
        "\n",
        "**Computing Average Precision (AP)**\n",
        "```python\n",
        "# Compute AP for a single query with a ranked list of images\n",
        "query_name = \"100900.jpg\"\n",
        "ranked_list = [\"100901.jpg\", \"100902.jpg\"]\n",
        "ap = dataset.compute_AP(query_name, ranked_list)\n",
        "print(f\"Average Precision (AP) for {query_name}: {ap}\")\n",
        "```\n",
        "\n",
        "**Computing Mean Average Precision (mAP)**\n",
        "```python\n",
        "# Compute mAP for multiple queries\n",
        "ranked_dict = {\n",
        "    \"100900.jpg\": [\"100901.jpg\", \"100902.jpg\"],\n",
        "    \"101000.jpg\": [\"101001.jpg\"]\n",
        "}\n",
        "map_score = dataset.compute_mAP(ranked_dict)\n",
        "print(f\"Mean Average Precision (mAP): {map_score}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a3e78f-6bf2-4463-808a-11f3207fa63b",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## **Global Descriptors**\n",
        "\n",
        "Once our development environment is set up, we can begin working on the assignment. In this section, we will explore and evaluate various global descriptors to build a simple image retrieval system.\n",
        "\n",
        "### General Framework\n",
        "\n",
        "To streamline our workflow, we will begin by developing essential utility functions and classes. Start by implementing a [Python class]((https://docs.python.org/3/tutorial/classes.html)) named `CBIR` in `assignment1.py`, which will encapsulate the core functionalities of a CBIR system. The class should adhere to the provided method descriptions. Assume the existence of a function, `desc_func`, that computes a global descriptor for a given image. This design enables the use of the same class with different descriptor methods, as we will explore later in the assignment.\n",
        "\n",
        "### Color Histograms\n",
        "- Let's begin with histograms. In the `assignment1.py` file, implement the function `compute_1d_color_hist` to compute a descriptor based on a 1D color histogram. The final image descriptor will be the concatenation of the **normalized** color histograms from the three individual channels (B, G, and R).\n",
        "\n",
        "- Next, implement the function called `compute_2d_color_hist` to compute a global descriptor for the image using 2D color histograms. The image descriptor will be the concatenation of the **normalized** histograms derived from the three possible combinations of color channels (B/G, B/R, and G/R).\n",
        "\n",
        "> **Useful functions**: [cv2.calcHist](https://docs.opencv.org/4.2.0/d6/dc7/group__imgproc__hist.html), [np.histogram](https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html)\n",
        "\n",
        "Check your implementations using the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f4803e-28d0-417f-bc28-289864640d86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the dataset handler\n",
        "dataset = HolidaysDatasetHandler(dataset_dir, load_features=False) # We don't need features now\n",
        "\n",
        "# Get an image from the dataset\n",
        "image_name = \"100900.jpg\"\n",
        "image = dataset.get_image(image_name)\n",
        "\n",
        "# Bins: 32\n",
        "h = a1.compute_1d_color_hist(image, 32);\n",
        "assert h.shape == (96, )\n",
        "\n",
        "# Bins: 16\n",
        "h = a1.compute_1d_color_hist(image, 16);\n",
        "assert h.shape == (48, )\n",
        "\n",
        "# Bins: 8\n",
        "h = a1.compute_1d_color_hist(image, 8);\n",
        "assert h.shape == (24, )\n",
        "\n",
        "# Bins: 32\n",
        "h = a1.compute_2d_color_hist(image, 32);\n",
        "assert h.shape == (3072, )\n",
        "\n",
        "# Bins: 16\n",
        "h = a1.compute_2d_color_hist(image, 16);\n",
        "assert h.shape == (768, )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51acd6b7-42b2-4247-a5aa-373e9cf796de",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "**Task 1:**\n",
        "\n",
        "Using the `HolidaysDatasetHandler` and `CBIR` classes, your task in the following cell is to compute the **mean Average Precision (mAP)** of this image retrieval system on this dataset. The image descriptors should be calculated using **1D histograms** with **8 bins**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78e56a13-5411-4d5e-acef-b6e1130863ab",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Fill this variable with the resulting mAP\n",
        "mAP_hist_1d = 0.0\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "# -----\n",
        "\n",
        "print('Mean Average Precision (mAP): %.5f' % mAP_hist_1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f8a13d6-60e3-43a1-8ca5-67cad4394742",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "**Task 2:**\n",
        "\n",
        "Study the effect of changing the number of bins per histogram.\n",
        "\n",
        "*Write in the following the code required to answer the question. You may add more code or markdown cells as needed.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01d193a4-3c46-4835-9c1b-907db2859e1b",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "**Task 3:**\n",
        "\n",
        "Again, using the `HolidaysDatasetHandler` and `CBIR` classes, your task in the following cell is to compute the **mean Average Precision (mAP)** of this image retrieval system on this dataset. However, this time the image descriptors should be calculated using **2D histograms** with **8 bins**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ef2b9c-92a8-44ce-9db9-a167d2f478c1",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Fill this variable with the resulting mAP\n",
        "mAP_hist_2d = 0.0\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "# -----\n",
        "\n",
        "print('Mean Average Precision (mAP): %.5f' % mAP_hist_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa48f73-81cf-4ba6-8af7-ae08a673f6f5",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "**Task 4:**\n",
        "\n",
        "As for the 1D case, study the effect of changing the number of bins per histogram.\n",
        "\n",
        "*Write in the following the code required to answer the question. You may add more code or markdown cells as needed.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2b6881c-8369-4301-b49e-950c7f936caf",
      "metadata": {},
      "source": [
        "**Task 5:**\n",
        "\n",
        "What other options can be implemented to improve the quality of retrieval using histograms? Experiment with some of them.\n",
        "\n",
        "*Write in the following the code required to answer the question. You may add more code or markdown cells as needed.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1a647cb-c46d-4452-bb4a-b2871f8c9a72",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "### Local Binary Patterns (LBP)\n",
        "\n",
        "Let's try LBP. Implement the function called `compute_lbp_descriptor` to calculate an image descriptor based on LBP. The descriptor should use the *rotation-invariant and uniform (RIU)* version of the algorithm. The final image descriptor will be the normalized histogram of the resulting LBP image.\n",
        "\n",
        "> **Useful functions**: [skimage.feature.local_binary_pattern](https://scikit-image.org/docs/dev/api/skimage.feature.html?highlight=lbp#skimage.feature.local_binary_pattern)\n",
        "\n",
        "Check your implementation using the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592d606f-64fe-42fa-887e-2e7426a24e61",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "h = a1.compute_lbp_descriptor(image, 8, 1);\n",
        "assert h.shape == (10, )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5d6d70f-aac7-4840-88df-8a41a04e6220",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "**Task 6:**\n",
        "\n",
        "As previously, your task in the following cell is to compute the **mean Average Precision (mAP)** of this image retrieval system on this dataset using the `HolidaysDatasetHandler` and `CBIR` classes. However, this time, the descriptors should be calculated using the `compute_lbp_descriptor` function using **8 neighbors for each pixel at a distance of 1 pixel**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdad292d-96ea-4281-9443-4cb87d320c54",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Fill this variable with the resulting mAP\n",
        "mAP_lbp = 0.0\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "# -----\n",
        "\n",
        "print('Mean Average Precision (mAP): %.5f' % mAP_lbp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8550a594-adf5-4bcd-9035-a042abd493a8",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "**Task 7:**\n",
        "\n",
        "Study the effect of changing the number of neighboring pixels $p$ and the radius $r$.\n",
        "\n",
        "*Write in the following the code required to answer the question. You may add more code or markdown cells as needed.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8841bbaf-1100-4c4b-b92e-50e3c2d101f2",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "**Task 8:**\n",
        "\n",
        "Implement a grid over the image, computing the LBP histogram for each cell. Then, concatenate these histograms to form a global descriptor for the image. Investigate the impact of this improvement on the retrieval performance.\n",
        "\n",
        "*Write in the following the code required to answer the question. You may add more code or markdown cells as needed.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9819f223-a57a-4826-a974-380086e10ff9",
      "metadata": {},
      "source": [
        "## **Local Descriptors**\n",
        "\n",
        "In this section, we will focus on retrieving images by leveraging local descriptors discussed during the class. These descriptors will help us analyze and compare images based on their distinct visual features.\n",
        "\n",
        "### Extracting Interest Points\n",
        "\n",
        "Let's start by writing the function `extract_interest_points` in `assignment1.py` to extract a set of keypoints from an image along with their corresponding descriptors. The function will take a parameter called `feat_type` to specify the method used for keypoint detection and description. This parameter can have one of the following string values:\n",
        "\n",
        "- **`SIFT`**: SIFT detector and descriptor.\n",
        "- **`FAST_BRIEF`**: FAST detector with BRIEF descriptor.\n",
        "- **`ORB`**: ORB detector and descriptor.\n",
        "\n",
        "\n",
        "> **Useful functions**: [cv2.SIFT_create()](https://docs.opencv.org/4.5.4/d7/d60/classcv_1_1SIFT.html#ad337517bfdc068ae0ba0924ff1661131), [cv2.FastFeatureDetector_create](https://docs.opencv.org/4.5.4/df/d74/classcv_1_1FastFeatureDetector.html#ab986f2ff8f8778aab1707e2642bc7f8e), [cv2.xfeatures2d.BriefDescriptorExtractor_create](https://docs.opencv.org/4.5.4/d1/d93/classcv_1_1xfeatures2d_1_1BriefDescriptorExtractor.html#ae3bc52666010fb137ab6f0d32de51f60), [cv2.ORB_create](https://docs.opencv.org/4.5.4/db/d95/classcv_1_1ORB.html#aeff0cbe668659b7ca14bb85ff1c4073b)\n",
        "\n",
        "By combining these tools, you can implement the function to handle different types of keypoint detection and description techniques effectively.\n",
        "\n",
        "Check your implementation using the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a5c6b0d-13e4-49ba-8bbf-83ae9c1ce80e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the dataset handler\n",
        "dataset = HolidaysDatasetHandler(dataset_dir, load_features=False) # We don't need features now\n",
        "\n",
        "# Get an image from the dataset\n",
        "image_name = \"100800.jpg\"\n",
        "image = dataset.get_image(image_name)\n",
        "\n",
        "# Check if the image was loaded correctly\n",
        "if image is None:\n",
        "    print(f\"Error: Could not load the image {image_name}.\")\n",
        "else:\n",
        "    # Extract interest points and descriptors\n",
        "    img_resized = cv2.resize(image, None, fx=0.25, fy=0.25, interpolation = cv2.INTER_CUBIC)\n",
        "    kps, des = a1.extract_interest_points(img_resized, 'SIFT', nfeats=2000, thresh=75)\n",
        "    \n",
        "    # Check if any keypoints were detected\n",
        "    if len(kps) == 0:\n",
        "        print(\"No keypoints were detected in the image.\")\n",
        "    else:\n",
        "        print(f\"{len(kps)} keypoints detected.\")\n",
        "        # Display the shape of the descriptors\n",
        "        print(f\"Descriptors shape: {des.shape}\")\n",
        "        \n",
        "        # Draw the keypoints on the image\n",
        "        sift_image = cv2.drawKeypoints(img_resized, kps, None)\n",
        "        \n",
        "        # Display the image without axes\n",
        "        plt.imshow(sift_image)  # Convert BGR to RGB for Matplotlib\n",
        "        plt.axis('off')  # Remove axes\n",
        "        plt.title('SIFT Keypoints')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf5377e-6499-4825-b6cb-55fda96c505b",
      "metadata": {},
      "source": [
        "### Matching Images\n",
        "\n",
        "- Next, implement the function called `find_matches` to match two sets of descriptors. Since descriptor types vary, we will use:\n",
        "    - **Brute-force matching** for binary descriptors (e.g., BRIEF, ORB).  \n",
        "    - **FLANN-based matching** for floating-point descriptors (e.g., SIFT).\n",
        "            \n",
        "-  Implement the function called `filter_matches` to refine the matching results using the **Nearest Neighbor Distance Ratio (NNDR) criterion**. This method helps remove ambiguous matches by ensuring that the best match is significantly better than the second-best match. This helps to discard ambiguous matches and improves robustness by reducing false positives.\n",
        "\n",
        "> **Useful links**: [cv2.BFMatcher_create](https://docs.opencv.org/4.5.4/d3/da1/classcv_1_1BFMatcher.html#ac6418c6f87e0e12a88979ea57980c020), [cv2.FlannBasedMatcher](https://docs.opencv.org/4.5.4/dc/de2/classcv_1_1FlannBasedMatcher.html#a7b17083dda906384465a32952c1bbe3b), [Feature Matching](https://docs.opencv.org/4.5.4/dc/dc3/tutorial_py_matcher.html)\n",
        "\n",
        "### Filtering Matches\n",
        "\n",
        "Implement the function called `filter_matches` to refine the matching results using the **Nearest Neighbor Distance Ratio (NNDR) criterion**. This method helps remove ambiguous matches by ensuring that the best match is significantly better than the second-best match.\n",
        "\n",
        "Check your implementation of these functions using the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78317f1f-19f8-4a69-9197-0d6e628cbdf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the dataset handler\n",
        "dataset = HolidaysDatasetHandler(dataset_dir, load_features=False)  # We don't need features now\n",
        "\n",
        "# Get images from the dataset\n",
        "image1_name = \"100100.jpg\"\n",
        "image2_name = \"100101.jpg\"\n",
        "\n",
        "image1 = dataset.get_image(image1_name)\n",
        "image2 = dataset.get_image(image2_name)\n",
        "\n",
        "# Check if images were loaded correctly\n",
        "if image1 is None or image2 is None:\n",
        "    print(\"Error: One or both images could not be loaded.\")\n",
        "else:\n",
        "    # Extract keypoints and descriptors for both images\n",
        "    kps_q, des_q = a1.extract_interest_points(image1, 'SIFT', nfeats=2000, thresh=25)\n",
        "    kps_t, des_t = a1.extract_interest_points(image2, 'SIFT', nfeats=2000, thresh=25)\n",
        "\n",
        "    # Check if descriptors were extracted properly\n",
        "    if des_q is None or des_t is None or len(des_q) == 0 or len(des_t) == 0:\n",
        "        print(\"Error: No descriptors found in one or both images.\")\n",
        "    else:\n",
        "        # Find and filter matches\n",
        "        raw_matches = a1.find_matches(des_q, des_t)\n",
        "        print(f\"Raw matches found: {len(raw_matches)}\")\n",
        "\n",
        "        matches = a1.filter_matches(raw_matches, ratio=0.6)\n",
        "        print(f\"Filtered matches: {len(matches)}\")\n",
        "\n",
        "        # Draw matches\n",
        "        img_matches = cv2.drawMatches(\n",
        "            image1, kps_q, image2, kps_t, matches, None, \n",
        "            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
        "        )\n",
        "\n",
        "        # Display the result\n",
        "        plt.imshow(img_matches)\n",
        "        plt.axis('off')  # Remove axes for a cleaner display\n",
        "        plt.title('Correct Matches')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebb1493-060b-4a20-a46a-f0f741cf12eb",
      "metadata": {},
      "source": [
        "## Evaluating Performance\n",
        "\n",
        "Using the `HolidaysDatasetHandler` and the functions you have implemented, your task is to complete the `evaluate` function in `assignment1.py` to compute the **mean Average Precision (mAP)** for an image retrieval system based on **local image features**. This time, images will be compared using **local feature descriptors**. Given a query image, the closest match in the database will be the **image with the highest number of matches after filtering**. You should apply a **threshold** to determine the minimum number of matches required for an image to be considered a **correct match**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c6d7cda-c5fc-4e35-9a69-80be81f62fad",
      "metadata": {},
      "source": [
        "**Task 9:**\n",
        "\n",
        "In the following cell, your task is to compute the **mean Average Precision (mAP)** for the image retrieval system using the `evaluate()` function. The goal is to evaluate the retrieval system's performance for each method (SIFT, FAST/BRIEF, and ORB) and fine-tune their respective parameters to achieve the best possible mAP.\n",
        "\n",
        "For each method:\n",
        "1. **SIFT**: Adjust parameters such as the number of features (`nfeats`) and the threshold (`thresh`).\n",
        "2. **FAST/BRIEF**: Experiment with the number of keypoints, thresholds, and descriptors.\n",
        "3. **ORB**: Modify the number of features (`nfeats`) and other relevant settings.\n",
        "\n",
        "Your objective is to achieve the highest mAP score by optimizing these parameters for each method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd61982-db41-4495-b291-3350e8d40fcd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill these variables with the best resulting mAPs\n",
        "best_mAP_SIFT = 0.0\n",
        "best_mAP_FASTBRIEF = 0.0\n",
        "best_mAP_ORB = 0.0\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "# -----\n",
        "\n",
        "# Print the best mAPs for each method\n",
        "print('Best mAP (SIFT): %.5f' % best_mAP_SIFT)\n",
        "print('Best mAP (FAST/BRIEF): %.5f' % best_mAP_FASTBRIEF)\n",
        "print('Best mAP (ORB): %.5f' % best_mAP_ORB)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189bf13a-4b1d-43c3-8de3-c49bd0681f45",
      "metadata": {},
      "source": [
        "**Task 10:**\n",
        "\n",
        "Compare the three methods in terms of accuracy. Which one is the most accurate? Which one is the least accurate?\n",
        "\n",
        "*Write in the following the code required to answer the question. You may add more code or markdown cells as needed.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe690a43-6e31-4d8c-99fa-0d36750b0dc2",
      "metadata": {},
      "source": [
        "**Task 11:**\n",
        "\n",
        "Compare the three methods in terms of detection and description speed. Which one is the fastest? Which one is the slowest?\n",
        "\n",
        "> **Hints**:\n",
        "> - [Here](https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html) you can find a useful resource on timing in Jupyter Notebooks.\n",
        "> - You can also consider using the `time` module.\n",
        "> - To provide a measure independent of the number of features detected, you can compute, for example, features per millisecond (features/ms).\n",
        "\n",
        "*Write in the following the code required to answer the question. You may add more code or markdown cells as needed.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45841553-6b9f-4325-8394-0571c8bc799b",
      "metadata": {},
      "source": [
        "## Submitting Your Work\n",
        "\n",
        "**Important**: Please ensure that the notebook has been run and that the **cell outputs are visible**.\n",
        "\n",
        "**Important**: Additionally, make sure you have filled in the names at the beginning of the notebook and the **ID** variable in the following cell.\n",
        "\n",
        "Once you have completed the necessary code and are satisfied with your solution, **save your notebook** and run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6292068e-cb71-49dd-9521-6ce1e2cdd1f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "ID = '99999999R' # Your DNI or NIE\n",
        "\n",
        "zip_filename = ID + '_A1.zip'\n",
        "zf = zipfile.ZipFile(zip_filename, mode = 'w')\n",
        "\n",
        "zf.write('11762_Image_Description.ipynb');\n",
        "zf.write('assignment1.py');\n",
        "zf.write('holidays_dataset_handler.py');\n",
        "\n",
        "zf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68826c74-b400-457b-926f-926a463d544d",
      "metadata": {},
      "source": [
        "This will generate a zip file of your code called `ID_A1.zip` in the same directory of the assignment. This is the file that you must upload to [Aula Digital](https://ad.uib.es/) to submit your work!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f0599ac-3c54-4be7-b371-7947b2104f04",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "&copy; Emilio Garcia-Fidalgo, University of the Balearic Islands"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}